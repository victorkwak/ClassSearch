{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of FastText model using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras import callbacks\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('data/cs_subs_train.csv')\n",
    "    val = pd.read_csv('data/cs_subs_val.csv')\n",
    "    test = pd.read_csv('data/cs_subs_test.csv')\n",
    "    \n",
    "    X_train, y_train = train['title'], train['subreddit']\n",
    "    X_val, y_val = val['title'], val['subreddit']\n",
    "    X_test, y_test = test['title'], test['subreddit']\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for i in range(len(new_list) - ngram_range + 1):\n",
    "            for ngram_value in range(2, ngram_range + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "def create_indices(X, ngram_range, max_features):\n",
    "    ngram_set = set()\n",
    "    for input_list in X:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "        \n",
    "    return token_indice, max_features\n",
    "\n",
    "def preprocess(X, y, train=False, ngram_range=2, max_len=30, max_features=1000, **kwargs):\n",
    "    \n",
    "    if train:\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(X)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "        \n",
    "        label_binarizer = LabelBinarizer()\n",
    "        y = label_binarizer.fit_transform(y)\n",
    "        \n",
    "        token_indice = None\n",
    "    else:\n",
    "        tokenizer = kwargs['tokenizer']\n",
    "        label_encoder = kwargs['label_encoder']\n",
    "        label_binarizer = kwargs['label_binarizer']\n",
    "        \n",
    "        y = label_encoder.transform(y)\n",
    "        y = label_binarizer.transform(y)\n",
    "        \n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    \n",
    "    if ngram_range > 1:\n",
    "        if train:\n",
    "            token_indice, max_features = create_indices(X, ngram_range, max_features)\n",
    "        else:\n",
    "            token_indice = kwargs['token_indice']\n",
    "        \n",
    "        X = add_ngram(X, token_indice, ngram_range)\n",
    "        \n",
    "    X = sequence.pad_sequences(X, maxlen=max_len)\n",
    "    \n",
    "    if train:\n",
    "        return X, y, tokenizer, label_encoder, label_binarizer, token_indice, max_features\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def top_n_accuracy(y_true, probs, n=5):\n",
    "    y_true_decoded = []\n",
    "    for label in y_true:\n",
    "        y_true_decoded.append(np.argmax(label))\n",
    "    \n",
    "    top_n_list = []\n",
    "    for prob in probs:\n",
    "        top_n_list.append(np.argsort(-prob)[:n])\n",
    "    predictions = []\n",
    "    for prediction, top_n in zip(y_true_decoded, top_n_list):\n",
    "        predictions.append(int(prediction in top_n))\n",
    "    return np.sum(predictions) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data into train (60%), val (20%), and test (20%) and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59364,)\n",
      "(19788,)\n",
      "(19789,)\n",
      "(59364,)\n",
      "(19788,)\n",
      "(19789,)\n",
      "max_features: 229413\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=17)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=31)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "ngram_range = 2\n",
    "max_features = 10000\n",
    "max_len = 30\n",
    "\n",
    "X_train, y_train, tokenizer, label_encoder, label_binarizer, token_indice, max_features = preprocess(\n",
    "    X_train, y_train, train=True, ngram_range=ngram_range, max_features=max_features, max_len=max_len)\n",
    "\n",
    "processors = {\n",
    "    'tokenizer': tokenizer,\n",
    "    'label_binarizer': label_binarizer,\n",
    "    'label_encoder': label_encoder,\n",
    "    'token_indice': token_indice\n",
    "}\n",
    "\n",
    "X_val, y_val = preprocess(\n",
    "    X_val, y_val, ngram_range=ngram_range, max_len=max_len, **processors)\n",
    "\n",
    "X_test, y_test = preprocess(\n",
    "    X_test, y_test, ngram_range=ngram_range, max_len=max_len, **processors)\n",
    "\n",
    "print('max_features:', max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59364 samples, validate on 19789 samples\n",
      "Epoch 1/1000\n",
      "59364/59364 [==============================] - 9s 146us/step - loss: 4.3022 - acc: 0.0730 - val_loss: 4.0610 - val_acc: 0.0907\n",
      "Epoch 2/1000\n",
      "59364/59364 [==============================] - 7s 125us/step - loss: 3.7602 - acc: 0.1840 - val_loss: 3.5441 - val_acc: 0.2481\n",
      "Epoch 3/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 3.0788 - acc: 0.3725 - val_loss: 3.0152 - val_acc: 0.3670\n",
      "Epoch 4/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 2.4278 - acc: 0.5156 - val_loss: 2.6565 - val_acc: 0.4282\n",
      "Epoch 5/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 1.9282 - acc: 0.6164 - val_loss: 2.4388 - val_acc: 0.4583\n",
      "Epoch 6/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 1.5464 - acc: 0.6928 - val_loss: 2.3019 - val_acc: 0.4782\n",
      "Epoch 7/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 1.2448 - acc: 0.7578 - val_loss: 2.2140 - val_acc: 0.4854\n",
      "Epoch 8/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 1.0024 - acc: 0.8106 - val_loss: 2.1555 - val_acc: 0.4955\n",
      "Epoch 9/1000\n",
      "59364/59364 [==============================] - 7s 124us/step - loss: 0.8080 - acc: 0.8501 - val_loss: 2.1227 - val_acc: 0.5000\n",
      "Epoch 10/1000\n",
      "59364/59364 [==============================] - 7s 125us/step - loss: 0.6535 - acc: 0.8800 - val_loss: 2.1019 - val_acc: 0.5041\n",
      "Epoch 11/1000\n",
      "59364/59364 [==============================] - 7s 125us/step - loss: 0.5317 - acc: 0.9028 - val_loss: 2.0962 - val_acc: 0.5062\n",
      "Epoch 12/1000\n",
      "59364/59364 [==============================] - 7s 115us/step - loss: 0.4357 - acc: 0.9192 - val_loss: 2.0987 - val_acc: 0.5073\n",
      "Epoch 13/1000\n",
      "59364/59364 [==============================] - 7s 119us/step - loss: 0.3601 - acc: 0.9326 - val_loss: 2.1088 - val_acc: 0.5057\n",
      "Epoch 14/1000\n",
      "59364/59364 [==============================] - 7s 114us/step - loss: 0.3040 - acc: 0.9460 - val_loss: 2.1096 - val_acc: 0.5061\n",
      "Epoch 15/1000\n",
      "59364/59364 [==============================] - 7s 114us/step - loss: 0.2982 - acc: 0.9471 - val_loss: 2.1114 - val_acc: 0.5061\n",
      "Epoch 16/1000\n",
      "59364/59364 [==============================] - 7s 114us/step - loss: 0.2922 - acc: 0.9479 - val_loss: 2.1130 - val_acc: 0.5060\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embedding_dims = 100\n",
    "epochs = 1000\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=X_train.shape[1]))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                         min_delta=0.001,\n",
    "                                         patience=5,\n",
    "                                         mode='min')\n",
    "\n",
    "get_best = callbacks.ModelCheckpoint(monitor='val_loss',\n",
    "                                     filepath='models/keras_fasttext.hdf5',\n",
    "                                     save_best_only=True)\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                        patience=1,\n",
    "                                        factor=0.00001,\n",
    "                                        min_lr=0.0001)\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping, get_best, reduce_lr],\n",
    "          validation_data=[X_val, y_val])\n",
    "\n",
    "model = load_model('models/keras_fasttext.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77785638486027586"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba(X_val)\n",
    "\n",
    "top_n_accuracy(y_val, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
