{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of \"simpler\" models using Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('data/cs_subs_train.csv')\n",
    "    val = pd.read_csv('data/cs_subs_val.csv')\n",
    "    test = pd.read_csv('data/cs_subs_test.csv')\n",
    "    \n",
    "    X_train, y_train = train['title'], train['subreddit']\n",
    "    X_val, y_val = val['title'], val['subreddit']\n",
    "    X_test, y_test = test['title'], test['subreddit']\n",
    "    \n",
    "    label_encoder = pickle.load(open('pickles/label_encoder.pkl', 'rb'))\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, label_encoder\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, label_encoder = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_val_vectors = vectorizer.transform(X_val)\n",
    "X_test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_accuracy(y_true, probs, n=5):\n",
    "    \"\"\"\n",
    "    Returns the top N accuracy for the classifier i.e., if the correct label is\n",
    "    within the top N most likely labels according to the classifier.\n",
    "    \"\"\"\n",
    "    top_n_list = []\n",
    "    for prob in probs:\n",
    "        top_n_list.append(np.argpartition(prob, -n)[-n:])    \n",
    "    predictions = []\n",
    "    for prediction, top_n in zip(y_true, top_n_list):\n",
    "        predictions.append(int(prediction in top_n))\n",
    "    predictions = np.array(predictions)\n",
    "    return predictions.sum() / y_true.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "bf = SelectPercentile(percentile=30)\n",
    "X_train_nb = bf.fit_transform(X_train_vectors, y_train)\n",
    "X_val_nb = bf.transform(X_val_vectors)\n",
    "nb.fit(X_train_nb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy:\n",
      " 0.30009158441\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.18      0.98      0.31      1352\n",
      "          1       0.70      0.39      0.50       193\n",
      "          3       0.00      0.00      0.00       115\n",
      "          4       0.83      0.02      0.05       216\n",
      "          5       1.00      0.04      0.08       126\n",
      "          6       0.00      0.00      0.00       102\n",
      "          7       1.00      0.15      0.26        92\n",
      "          9       0.00      0.00      0.00        66\n",
      "         10       0.00      0.00      0.00        13\n",
      "         12       1.00      0.02      0.04        90\n",
      "         13       0.00      0.00      0.00        14\n",
      "         14       0.00      0.00      0.00        45\n",
      "         15       0.00      0.00      0.00        86\n",
      "         16       0.00      0.00      0.00        12\n",
      "         17       0.00      0.00      0.00        20\n",
      "         18       0.00      0.00      0.00        16\n",
      "         19       0.00      0.00      0.00        40\n",
      "         20       0.48      0.08      0.14       447\n",
      "         21       0.60      0.05      0.10       286\n",
      "         22       0.88      0.08      0.15        86\n",
      "         23       0.00      0.00      0.00        53\n",
      "         24       0.00      0.00      0.00        56\n",
      "         25       0.48      0.39      0.43       472\n",
      "         27       0.69      0.27      0.39       393\n",
      "         28       0.92      0.35      0.51       327\n",
      "         29       0.00      0.00      0.00        98\n",
      "         30       0.92      0.33      0.49       261\n",
      "         31       0.81      0.22      0.34       160\n",
      "         32       0.82      0.37      0.51       260\n",
      "         33       0.00      0.00      0.00        60\n",
      "         34       0.00      0.00      0.00        23\n",
      "         37       0.59      0.33      0.42       306\n",
      "         38       0.00      0.00      0.00        62\n",
      "         39       0.00      0.00      0.00       104\n",
      "         40       0.00      0.00      0.00        61\n",
      "         41       0.00      0.00      0.00        89\n",
      "         42       0.00      0.00      0.00        69\n",
      "         43       0.33      0.01      0.01       158\n",
      "         44       0.00      0.00      0.00        55\n",
      "         45       0.00      0.00      0.00        96\n",
      "         46       0.88      0.10      0.18       152\n",
      "         47       0.00      0.00      0.00         7\n",
      "         48       1.00      0.01      0.02        86\n",
      "         49       0.31      0.86      0.46       768\n",
      "         50       0.67      0.01      0.02       171\n",
      "         51       0.00      0.00      0.00       113\n",
      "         52       0.90      0.55      0.68       237\n",
      "         53       0.00      0.00      0.00         9\n",
      "         55       0.00      0.00      0.00        16\n",
      "         56       1.00      0.33      0.50       180\n",
      "         57       0.73      0.16      0.26       222\n",
      "         58       1.00      0.01      0.02        91\n",
      "         59       1.00      0.05      0.10       117\n",
      "         60       0.00      0.00      0.00        35\n",
      "         61       0.79      0.16      0.27       116\n",
      "         63       0.65      0.36      0.46       171\n",
      "         64       1.00      0.05      0.09        43\n",
      "         65       0.00      0.00      0.00        45\n",
      "         66       0.00      0.00      0.00        21\n",
      "         67       0.00      0.00      0.00        60\n",
      "         68       0.67      0.03      0.05        70\n",
      "         69       0.00      0.00      0.00        58\n",
      "         70       0.56      0.05      0.09       210\n",
      "         72       0.29      0.09      0.14       516\n",
      "         73       0.85      0.27      0.40       211\n",
      "         74       0.00      0.00      0.00        26\n",
      "         76       0.00      0.00      0.00        61\n",
      "         77       0.56      0.74      0.64       471\n",
      "         78       0.57      0.13      0.21       231\n",
      "         79       0.50      0.03      0.06       348\n",
      "         80       0.00      0.00      0.00        24\n",
      "         81       0.66      0.41      0.50       313\n",
      "         82       0.43      0.05      0.09       281\n",
      "         83       0.38      0.29      0.33       409\n",
      "         84       0.00      0.00      0.00        51\n",
      "         85       0.93      0.10      0.18       137\n",
      "         86       0.00      0.00      0.00       105\n",
      "         89       0.24      0.09      0.14       373\n",
      "         90       0.18      0.68      0.29       591\n",
      "         92       0.47      0.14      0.22       389\n",
      "         93       0.07      0.00      0.00       395\n",
      "         94       0.00      0.00      0.00        67\n",
      "         95       0.23      0.74      0.35       771\n",
      "         96       0.00      0.00      0.00        18\n",
      "         97       0.81      0.26      0.39       201\n",
      "         98       0.00      0.00      0.00        35\n",
      "        100       0.00      0.00      0.00        39\n",
      "        101       0.00      0.00      0.00        54\n",
      "        102       0.77      0.44      0.56       446\n",
      "        103       0.00      0.00      0.00        30\n",
      "        104       0.44      0.10      0.16       150\n",
      "        105       0.00      0.00      0.00        13\n",
      "        106       0.00      0.00      0.00       119\n",
      "        107       0.00      0.00      0.00        50\n",
      "        108       0.00      0.00      0.00        93\n",
      "        110       1.00      0.03      0.05        79\n",
      "        111       0.00      0.00      0.00        26\n",
      "        113       0.82      0.06      0.11       149\n",
      "        114       0.85      0.38      0.53       217\n",
      "        115       0.68      0.25      0.37       198\n",
      "        116       0.00      0.00      0.00        13\n",
      "        117       0.00      0.00      0.00        10\n",
      "        118       0.60      0.09      0.15       139\n",
      "        119       0.00      0.00      0.00        20\n",
      "        120       0.87      0.59      0.70       264\n",
      "        121       1.00      0.03      0.06        95\n",
      "        123       0.00      0.00      0.00        18\n",
      "        124       0.64      0.08      0.14       350\n",
      "        125       0.75      0.06      0.11       153\n",
      "        126       0.00      0.00      0.00        23\n",
      "        129       0.00      0.00      0.00        26\n",
      "        130       0.37      0.04      0.07       260\n",
      "        131       0.15      0.31      0.20       547\n",
      "        132       0.61      0.14      0.23       396\n",
      "        133       0.00      0.00      0.00        27\n",
      "        134       0.00      0.00      0.00        36\n",
      "        135       0.00      0.00      0.00        12\n",
      "\n",
      "avg / total       0.46      0.30      0.25     19654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorkwak/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nb_predictions = nb.predict(X_val_nb)\n",
    "nb_probs = nb.predict_proba(X_val_nb)\n",
    "\n",
    "print('Top 1 accuracy:\\n', accuracy_score(y_val, nb_predictions))\n",
    "print(classification_report(y_val, nb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine.\n",
    "#### Support vector machines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy:\n",
      " 0.500712323191\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.86      0.77      1352\n",
      "          1       0.63      0.70      0.66       193\n",
      "          3       0.15      0.07      0.09       115\n",
      "          4       0.40      0.31      0.35       216\n",
      "          5       0.48      0.50      0.49       126\n",
      "          6       0.15      0.07      0.09       102\n",
      "          7       0.89      0.71      0.79        92\n",
      "          9       0.17      0.14      0.15        66\n",
      "         10       0.29      0.15      0.20        13\n",
      "         12       0.65      0.59      0.62        90\n",
      "         13       0.82      1.00      0.90        14\n",
      "         14       0.50      0.33      0.40        45\n",
      "         15       0.38      0.42      0.40        86\n",
      "         16       0.33      0.08      0.13        12\n",
      "         17       0.81      0.85      0.83        20\n",
      "         18       0.50      0.19      0.27        16\n",
      "         19       0.72      0.70      0.71        40\n",
      "         20       0.28      0.22      0.25       447\n",
      "         21       0.47      0.35      0.40       286\n",
      "         22       0.51      0.58      0.54        86\n",
      "         23       0.32      0.36      0.34        53\n",
      "         24       0.08      0.02      0.03        56\n",
      "         25       0.51      0.54      0.52       472\n",
      "         27       0.58      0.55      0.57       393\n",
      "         28       0.63      0.57      0.60       327\n",
      "         29       0.47      0.23      0.31        98\n",
      "         30       0.74      0.70      0.72       261\n",
      "         31       0.61      0.72      0.66       160\n",
      "         32       0.76      0.77      0.76       260\n",
      "         33       0.30      0.33      0.31        60\n",
      "         34       0.35      0.30      0.33        23\n",
      "         37       0.49      0.62      0.55       306\n",
      "         38       0.31      0.21      0.25        62\n",
      "         39       0.03      0.01      0.01       104\n",
      "         40       0.19      0.13      0.16        61\n",
      "         41       0.23      0.11      0.15        89\n",
      "         42       0.59      0.38      0.46        69\n",
      "         43       0.32      0.24      0.27       158\n",
      "         44       0.56      0.40      0.47        55\n",
      "         45       0.10      0.04      0.06        96\n",
      "         46       0.51      0.52      0.52       152\n",
      "         47       0.00      0.00      0.00         7\n",
      "         48       0.46      0.45      0.46        86\n",
      "         49       0.50      0.74      0.60       768\n",
      "         50       0.28      0.15      0.19       171\n",
      "         51       0.38      0.39      0.38       113\n",
      "         52       0.64      0.81      0.72       237\n",
      "         53       0.57      0.89      0.70         9\n",
      "         55       0.75      0.75      0.75        16\n",
      "         56       0.89      0.79      0.84       180\n",
      "         57       0.51      0.55      0.53       222\n",
      "         58       0.50      0.48      0.49        91\n",
      "         59       0.73      0.73      0.73       117\n",
      "         60       0.08      0.03      0.04        35\n",
      "         61       0.79      0.72      0.75       116\n",
      "         63       0.54      0.60      0.57       171\n",
      "         64       0.80      0.84      0.82        43\n",
      "         65       0.69      0.40      0.51        45\n",
      "         66       0.77      0.48      0.59        21\n",
      "         67       0.66      0.67      0.66        60\n",
      "         68       0.42      0.60      0.50        70\n",
      "         69       0.37      0.38      0.38        58\n",
      "         70       0.37      0.25      0.30       210\n",
      "         72       0.20      0.32      0.25       516\n",
      "         73       0.70      0.60      0.65       211\n",
      "         74       0.23      0.12      0.15        26\n",
      "         76       0.34      0.34      0.34        61\n",
      "         77       0.71      0.93      0.80       471\n",
      "         78       0.48      0.39      0.43       231\n",
      "         79       0.49      0.53      0.51       348\n",
      "         80       0.50      0.12      0.20        24\n",
      "         81       0.58      0.65      0.61       313\n",
      "         82       0.29      0.28      0.29       281\n",
      "         83       0.44      0.40      0.42       409\n",
      "         84       0.56      0.39      0.46        51\n",
      "         85       0.83      0.65      0.73       137\n",
      "         86       0.26      0.10      0.15       105\n",
      "         89       0.30      0.21      0.25       373\n",
      "         90       0.33      0.49      0.40       591\n",
      "         92       0.42      0.44      0.43       389\n",
      "         93       0.27      0.19      0.22       395\n",
      "         94       0.32      0.13      0.19        67\n",
      "         95       0.38      0.48      0.43       771\n",
      "         96       0.64      0.78      0.70        18\n",
      "         97       0.54      0.64      0.59       201\n",
      "         98       0.20      0.06      0.09        35\n",
      "        100       0.67      0.46      0.55        39\n",
      "        101       0.42      0.44      0.43        54\n",
      "        102       0.64      0.65      0.65       446\n",
      "        103       0.61      0.47      0.53        30\n",
      "        104       0.46      0.49      0.47       150\n",
      "        105       0.33      0.08      0.12        13\n",
      "        106       0.31      0.27      0.29       119\n",
      "        107       0.89      0.62      0.73        50\n",
      "        108       0.26      0.11      0.15        93\n",
      "        110       0.79      0.58      0.67        79\n",
      "        111       0.80      0.46      0.59        26\n",
      "        113       0.54      0.48      0.51       149\n",
      "        114       0.72      0.65      0.68       217\n",
      "        115       0.61      0.78      0.68       198\n",
      "        116       0.82      0.69      0.75        13\n",
      "        117       0.00      0.00      0.00        10\n",
      "        118       0.68      0.54      0.60       139\n",
      "        119       0.00      0.00      0.00        20\n",
      "        120       0.76      0.79      0.77       264\n",
      "        121       0.82      0.65      0.73        95\n",
      "        123       0.15      0.17      0.16        18\n",
      "        124       0.34      0.28      0.31       350\n",
      "        125       0.61      0.44      0.52       153\n",
      "        126       1.00      0.83      0.90        23\n",
      "        129       0.71      0.46      0.56        26\n",
      "        130       0.26      0.26      0.26       260\n",
      "        131       0.25      0.19      0.22       547\n",
      "        132       0.42      0.43      0.43       396\n",
      "        133       0.57      0.15      0.24        27\n",
      "        134       0.61      0.47      0.53        36\n",
      "        135       0.75      0.50      0.60        12\n",
      "\n",
      "avg / total       0.49      0.50      0.49     19654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorkwak/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm = LinearSVC(penalty='l2', loss='squared_hinge', multi_class='ovr', max_iter=1000)\n",
    "\n",
    "bf = SelectPercentile(percentile=30)\n",
    "X_train_svm = bf.fit_transform(X_train_vectors, y_train)\n",
    "X_val_svm = bf.transform(X_val_vectors)\n",
    "\n",
    "svm.fit(X_train_svm, y_train)\n",
    "\n",
    "svm_predictions = svm.predict(X_val_svm)\n",
    "print('Top 1 accuracy:\\n', accuracy_score(y_val, svm_predictions))\n",
    "print(classification_report(y_val, svm_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
